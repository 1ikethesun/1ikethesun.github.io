---
title:  "강화학습을 읽고 ... 1. 준비(1)"
excerpt: "모리무라 선생님의 강화학습을 읽고 요약하기 위한 포스트입니다."

categories:
  - rl
tags:
  - [강화학습, TIL]

toc: true
toc_sticky: true
 
date: 2024-04-25
last_modified_at: 2024-04-25
---

# 1.1. 강화학습이란?

__강화학습__(reinforcement learning; RL) : 최적의 의사결정 룰을 구하는 것을 목적으로 하는 학문이며, 지도학습, 비지도학습과 더불어 기계학습의 한 분야로서 분류되고 있다.

__강화학습의 최대의 특징__

보수라는 개념이 존재해, 보수의 기대값 등을 최대로 하는 방책(순차적 의사 결정 룰)을 학습한다.

__강화학습에서의 의사 결정의 특징__

 상태(현재의 상황)하에 행해져, 그 결과로서, 보수나 새로운 상태를 관측하고, 다시 의사결정을 행하는 것을 반복한다.  

순차적 의사 결정 문제 : 방책의 최적화 문제

### 예 1.1. 어느 소매점의 세일 결정 문제

목적이 "판매시기의 매출의 장기 평균을 최대"인 각 판매시기 마다의 세일의 실시 결정 문제를 생각해보자.



![](https://1ikethesun.github.io/assets/images/rl/rl1.jpg "예1")

위 그림은 low, mid, high의 3단계의 "고객 구매 욕구", 각 "고객 구매 욕구"의 "세일 실시 유무"에 따른 매출과 그 다음 어떠한 "고객 구매 욕구"가 되는지 보여주는 예시이다.

각 시기의 매출은 "고객 구매 욕구", "세일 실시 유무"에 의존하는 걸 알 수 있고, 강화학습적인 측면에서 구분하자면, 매출은 보수, "고객 구매 욕구"는 상태, "세일 실시 유무"는 행동으로 구분할 수 있다.

보수를 결정하는 방책은 상태에 의존하여, 결정론적(비확률적)인 방책은, 다음 4가지의 패턴을 들 수 있다.


방책 A : 계속 되는 세일, 평균 매출은 약 1로 수렴

방책 B : 세일을 일절 안함. 평균 매출은 약 3으로 수렴

방책 C : 구매 욕구가 mid일 때, 세일을 실시. 평균 매출은 약 2

$$ \dfrac{0+4}{2}=2 $$

방책 D :  구매 욕구가 high일 때, 세일을 실시, 평균 매출은 약 4

$$ \dfrac{0+2+10}{3}=4 $$

평균 매출(평균 보수)를 최대로 하는 방책은 D이고, 세일을 남발하지 않고 고객의 구매 욕구가 high일 때 세일을 실시해야한다는 것을 알 수 있다. 한편 근시안적으로 즉시적으로 매출을 올리는 것은 방책 A이나, 근시안적으로는 최선의 선택일지라도 평균 매출이란 장기적 평가에서는 최악의 선택임을 알 수 있다. 이처럼 강화 학습은 당장은 손해가 있더라도 종합적으로 이익을 얻어내는 방책을 학습하는 것을 목적으로 두고 있다.

## 1.2. 마르코프 결정 과정과 순차적 의사 결정 문제

### 1.2.1. 확률 과정과 마르코프성

<span style="color: red">수학적 확률(probabilty)</span> : 같은 조건 속에서 여러 번 반복 가능한 동일한 시행에 의해 발생하는 각 사상의 가능성을 모두 정량적으로 표현한 것으로 사상 $A$의 확률을 $Pr(A)$로 표기하고 이는 사상 $A$가 어떤 시행을 랜덤으로 했을 때 발생할 확률을 의미한다.

__확률 변수__(random variable) : 표본 공간의 각 원소에 하나의 실수를 대응시킬 때, 이 실수를 확률변수라 하며, 확률변수는 함수이다. (자세한건 확률과 통계에서)

확률 변수를 대문자 $X$로, 그에 해당하는 실현값을 소문자 $x$로 구별하고, 확률 변수의 얻어지는 값(실현값)의 집합을 $\mathcal{X}$로 표현한다.

예시로 주사위에 나올 수 있는 면을 확률 변수로 두면 확률은
$$
Pr(X=x) =\dfrac{1}{6}, \forall x \in \mathcal{X} \triangleq {\{1,2, \dots,6\}}
$$

로 표현할 수 있다. 

__확률 분포__(probability distribution ) : 확률 변수과 확률 간의 대응 관계를 의미하며, 간단히 __분포__ 라고도 한다.

![](https://1ikethesun.github.io/assets/images/rl/rl2-1.jpg "확률변수")

__확률 과정__(stochastic process) : 예를 들면, 주사위를 한 번만 던지는 것이 아닌 여러 번 반복해 던져서 나오는 눈을 차례대로 나열한 수열, 혹은 눈의 누적합의 수열처럼 "변수의 값이 시간과 함께 확률적으로 변화하는 __확률 변수의 계열__"을 의미한다. 따라서 확률 과정은 시간 스텝 $t$를 파라미터로 하여,  __${X_t, t \in \mathcal{T}}$__ 라고 쓰는 경우가 많다.   $\mathcal{t}$는 시간 스텝 $t$가 취할 수 있는 값의 집합으로, 연속 시간을 다룰 때에는 $\mathcal{T}$ 대신에 실수 집합 $\mathbb{R}$ 로 하는 경우도 있으나, 앞으로의 시간은 이산적인 점열로 만들어지는 집합을 상정해서 서술하겠다.

$\\{X_t, t \in \mathbb{N}\\} \triangleq X_1, X_2, \dots$ 
$($또는 $\\{X_t,t \in \mathbb{N_0}\\} \triangleq X_0, X_1, \dots)$

일반적인 확률 과정에서, 시간 스텝 $t$의 확률 변수 $X_t$가 $x \in \mathcal{X}$를 취할 확률은,

$$ Pr(X_t=x \mid X_1=x_1, \dots,X_{t-1}=x_{t-1}) $$

이고, 시간 스텝 $ t $ 이전의 모든 실현치의 의존한다. 

(여기서 
$ Pr(A \mid B) $
는 조건부확률을 의미한다.)

한편, 강한 제약이 걸린 더 단순한 확률 과정으로서, 각 확률 변수 $X_1, X_2, \dots$ 가 서로 독립이고 동일의 확률분포를 따르는 경우를 생각해보자. 이때, $X_1, X_2, \dots$는 __독립동일분포__(independent and identically distributed; i.i.d)를 따른다라고 하며, $ \forall x_1,x_2,\dots,x_{t-1},x \in \mathcal{X} $에 대해

$ Pr(X_t \mid X_1=x_1, X_2=x_2, \dots,X_{t-1}=x_{t-1})$
$=Pr(X_k=x), \forall k \in \mathbb{N} $

가 성립한다. 즉 전에 무슨 사건이 벌어졌던 간에 영향을 받지 않고 어떠한 시간대에서도 실현치 $x$ 가 발생할 확률은 동일하다는 의미이다.

즉, 데이터가 i.i.d.를 따른다면 데이터의 순서, 시계열성을 고려하지 않고, 표준적인 패턴 인식이나 기계학습의 방법을 이용할 수 있기에 다루기 쉽다. 허나, 대다수의 많은 의사결정의 문제에 대해 i.i.d.의 가정을 두는 것은 불가능하기에, 강화학습에서는 i.i.d.보다 더 약한 제약인 __마르코프性__(Markov property)을 가정한다(i.i.d.와 마르코프성의 정의로부터 어느 확률과정이 i.i.d.를 따른다면, 마르코프性도 만족하지만 역은 성립하지 않는다). 

__마르코프性__ : 미래의 확률변수의 조건부 확률분포가 현 시간 스텝 $t$의 실현값 $x_t$에만 의존하고, 그 전 확률변수들의 실현값에는 의존하지 않는 성질이다. 즉 현 시각의 실현값 $x_t$가 주어진다면, $t-1$이전의 확률변수의 실현값 $x_1, x_2,\dots,x_{t-1}$에는 의존하지 않는다. 즉 마르코프性이라는 특징을 갖는 확률과정은, $\forall t,k \in \mathbb{N}, x_1,\dots,x_t,x \in \mathcal{X}$에 대해

$ Pr(X_{t+k}=x \mid X_1=x_1,\dots,X_t=x_t)$
$ =Pr(X_{t+k}=x \mid X_t=x_t) $

을 만족한다. 

__상태전이확률__(state transition probability) : 확률변수 $X$를 __상태변수__ 라고 보면, $ Pr(X_{t+1}=x' \mid X_t=x) $
는 상태 $x$로부터 다음 스텝에서 상태 $x'$에 전이하는 확률을 표현한 것이기에, 이 확률을 상태전이 확률이라한다. 

__마르코프과정__(Markov process) : 마르코프性를 갖는 확률과정

__마르코프 체인__(Markov chain) : 마르코프 과정의 상태변수가 취할 수 있는 값이 이산적(유한 또는 가산)인 경우, 이 과정을 마르코프 체인이라 한다.

#### 예 1.2. 확률과정의 구체적인 예

주사위를 임의로 던지는 시행을 반복한다고 가정했을 때, 확률변수 $X_t, Y_t, Z_t$를 다음과 같이 정의하자.

(a) $X_t$ : $t$번째 나온 눈의 수

(b) $Y_t$ : $t$번째까지의 최대값

(c) $Z_t$ : $t$번째까지의 중앙값

| 주사위 던진 횟수 $t$ |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   | 9    |  10  |
| :------------------: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | ---- | :--: |
|         (a)          |  3   |  1   |  2   |  2   |  4   |  1   |  6   |  3   | 4    |  2   |
|         (b)          |  3   |  3   |  3   |  3   |  4   |  4   |  6   |  6   | 6    |  6   |
|         (c)          |  3   |  2   |  2   |  2   |  2   |  2   |  2   | 2.5  | 3    | 2.5  |

여기서 (a)의 확률변수 $X_t$는 당연하게도 i.i.d.를 따른다는 것을 알 수 있다. 

또 (b)의 확률변수 $Y_t$는 $t$까지의 눈의 최대값이므로, $X_0=0$으로 두고, $\forall t \in \mathbb{N},i,j \in {1,2,\dots,6}$에 대해,
$$
Pr(X_{t+1}=i \mid X_t=j)= \begin{cases}\dfrac{1}{6}&\text{if}\;i>j,\\ \dfrac{j}{6}&\text{if}\;i=j,\\ 0&\text{if}\;i<j.\end{cases}
$$
와 같이 상태전이확률의 형태로 다음 스텝의 상태의 확률을 기술할 수 있다. 따라서 $Y_t$는 마르코프性을 만족한다는 것을 알 수 있다. 

한편 (c)의 확률변수 $Z_t$는 i.i.d.도 마르코프性도 만족하지 않는다.

이처럼 마르코프性은 강화학습을 생각하는 데에 있어서 엄청나게 중요한 특징이 된다. 왜냐하면, 만약 마르코프性이 성립하지 않는 임의의 확률과정을 학습의 대상으로 지정해버리면, 상태전이의 확률분포의 복잡성이 시간스텝 $t$에 대해 매우 복잡한 구조를 가지며, 시간에 따라 거대해져 일반적으로 다룰 수 없게 되기 때문이다. 따라서 강화학습을 실제 문제에 적용할 때에는, 강화학습법을 적용하기 전에, 대상의 시스템이 마르코프性를 만족하는 확률변수를 정의하는 등 확률과정을 주의 깊게 설계하는 것이 중요하다.
