---
title:  "강화학습을 읽고 ... 1. 준비(2)"
excerpt: "모리무라 선생님의 강화학습을 읽고 요약하기 위한 포스트입니다."

categories:
  - rl
tags:
  - [강화학습, TIL]

toc: true
toc_sticky: true
 
date: 2024-05-29
last_modified_at: 2024-05-29
---

# 1.2. 마르코프 결정 과정과 순차적 의사 결정 문제

## 1.2.2. 마르코프 결정 과정

__확률제어과정__(stochastic cotrol process) : __상태__(state)만 다루는 확률과정이 아닌, __행동__(action) 등을 추가한 __확률과정__이다.

강화학습에선 행동 선택 룰의 최적화를 구해야하므로, 확률제어과정을 알아보도록 하자.

__마르코프 결정과정__(Markov decision process; MDP) : 마르코프 체인에, __행동__(action)과 어떤 상태에서 고른 행동에 따른 __보수__(reward)를 추가한 __확률제어과정__이다.

$M \triangleq {\\{S, A, p_{s_{0}}, p_{\mathsf{T}}, g\\}}$. 마르코프 결정과정 M의 요소는 다음과 같다.

$ \bullet $유한상태집합 $\mathcal{S} \triangleq {\\{1, \dots, \vert \mathcal{S} \vert \\}} \ni s$

$ \bullet $ 유한행동집합 $\mathcal{A} \triangleq {\\{a^1, \dots, a^{\vert \mathcal{A} \vert}\\}} \ni a $

$ \bullet $ 초기상태확률함수 $p_{s_{0}}: \mathcal{S} \rightarrow [0,1] , p_{s_{0}}(s) \triangleq Pr(S_{0}=s)$

$ \bullet $ 상태전이확률함수 $p_{\mathsf{T}}: \mathcal{S} \times  \mathcal{S} \times \mathcal{A}  \rightarrow [0,1], p_{ \mathsf{T}}(s' \mid s,a)  \triangleq Pr(S_{t+1} = s'  \mid S_t = s, A_t = a),  \forall t  \in  \mathbb{N}_0 $

$  \bullet $ 보수함수 $g :  \mathcal{S}  \times  \mathcal{A}  \rightarrow  \mathbb{R}$

(여기서, 유한상태집합, 유한행동집합에 대한 이산 시간 마르코프 결정과정을 주로 다루지만, 연속상태공간, 연속행동공간, 연속시간의 마르코프 결정과정에 관한 강화학습의 연구도 활발히 이루어지고 있다. )

__마르코프 보수과정__(Markov reward process) : 마르코프 체인에 보수를 추가한 마르코프 과정, 혹은 $ \mathcal{A} = 1 $인 마르코프 결정과정을 의미한다.

보수 함수$g$의 정의역이 유한 집합의 곱집합이므로, 그에 상응하는 치역도 유한 집합이며, 이는 보수 함수$g$가 유계 함수임을 뜻한다. 따라서, 

$ \exists R_{max} \in \mathbb{R} $ s.t. 

$ \forall (s,a) \in \mathcal{S} \times \mathcal{A} \vert g(s, a) \vert \leqq R_{max} $ 

가 성립한다.

보수 집합 $\mathcal{R}$에 대해서 정의하면,

 $ \mathcal{R} \triangleq {\\{r \in \mathbb{R} : r = g(s,a), \exists (s,a) \in \mathcal{S} \times \mathcal{A}\\}} $

라 할 수 있고, 정의로부터 $ \vert \mathcal{R} \vert \leqq \vert \mathcal{S} \vert \vert \mathcal{A} \vert $ 가 성립한다.



행동집합은 상태에 의존하지 않는 단일의 집합 A로 다루겠다. 상태 s에 ㅢ해 선택 가능한 행동집합 $ A_s $ 이 다른 경우에도 

$ \mathcal{A}_1 \cup \dots \cup \mathcal{A}_{\vert \mathcal{S} \vert} = \mathcal{A} $ 라고 정의하는 것으로 지금까지 정의한 것에 대해 간단히 적용가능한 것을 알 수 있다. 또 보수함수에 대해서도, 간단함을 우선으로 현 시간스텝 $t$의 상태와 행동에 한해서 의존하는 $ g(S_t,A_t) $를 이용하겠다. 보다 일반적으론 행동 선택 뒤의 상태에도 의존하는 $g'(S_t,A_t,S_{t+1})$나 보수분포함수 $ Pr(R_t \leqq r \mid S_t = s, A_t = a) $등을 이용하는 경우도 있지만, 대부분의 경우 $g$와 같은 것으로 취급이 가능하다. 

__손실__(cost) : 보수에 기호 '$-$'를 붙인 것/

__방책__(policy) : 마르코프 결정과정에서의 입력이 되는 행동을 선택하는 법칙을 규정하는 함수.

