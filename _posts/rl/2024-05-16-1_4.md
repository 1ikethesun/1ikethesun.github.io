---

title:  "강화학습 1. 준비(4)"
excerpt: "강화학습 요약 노트"

categories:
  - rl
tags:
  - [강화학습, TIL]

toc: true
toc_sticky: true

date: 2024-05-16
last_modified_at: 2024-05-16

---

# 1.4. 순차적 의사결정 문제의 정식화

1.4.1. 순차적 의사결정 문제 설정

1.4.2. 마르코프 결정과정의 분류

1.4.3. 표준적인 목적함수를 해설해, 순차적 의사결정 문제를 정식화



## 1.4.1. 문제 설정

__순차적 의사결정 문제__ : 방책의 최적화 문제, 학습에서 조정가능한 것은 방책 $\pi$ 만을 한정하며, 환경 모델인 마르코프 결정과정 $M \triangleq {\mathcal{S}, \mathcal{A}, p_{s_0}, p_T, g}$ 는 강화학습을 적용하는 과제에 의해 결정되고, 시간불변이라 하자. 

혹시, 환경모델이 알려진 경우라면,  후술할 내용과 같이, 데이터가 없어도, 환경모델로부터 방책을 최적화하는 것이 가능하다. 따라서 데이터로부터 방책을 학습하는 경우랑 구별하는 경우가 많고, 환경모델로부터 최적방책을 구하는 것을, __학습__(learning)이라 하지 않고, __플래닝__(planning) 혹은 __플래닝문제__(planning problem)이라 부르는 경우가 많다.

![](https://1ikethesun.github.io/assets/images/rl/rl-1-2.jpg "예2")

__강화학습문제__(reinforcement learning problem) 을 앞으론 환경모델이 알려지지 않은 경우의 방책의 학습문제로서 다루겠다. 강화학습문제의 경우, 플래닝과는 다르게, 종래의 최적화 솔루션을 그대로 적용가능한 최적화 문제로서 정식화할 수 없고, 데이터(환경과의 상호작용의 결과)로부터, 학습을 필요로 한다.

강화학습문제의 설정으로서, 2개의 큰 설정이 존재한다. 1개는 __배치학습__(batch learning)이란 것으로, 종래의 기계학습과 닮은 설정으로, 환경과의 상호작용 등으로부터 얻어진 데이터가 대량으로 존재해, 그 데이터로부터 방책을 학습하는 것이다. 오프라인 학습이라고 불리기도 한다.

다른 1개는 __온라인학습__(online learning)이란 것으로, 데이터 수집 및 탐색, 데이터 활용이라는 2가지의 의사결정 전략이 있어, 이 전략들의 밸런스를 고려할 필요가 있다. 이 밸런스에 관한 것을 __탐색과 활용의 트레이드 오프__(exploration-exploitation trade-off)이라 한다.

$\cdot$ 데이터 수집 및 탐색 (exploration) : 데이터가 충분하지 않은 입장에서부터, 환경에 대한 불확실성을 줄이고, 새로운 발견이 가능하도록 행동을 선택하는 전략

$\cdot$ 데이터 활용 (exploitation) : 데이터가 이미 충분한 입장에서부터, 데이터로부터 최선이라 판단되는 행동을 선택하는 전략



## 1.4.2. 마르코프 결정과정의 단일화

대상으로 하는 순차적 의사결정 문제의 설정에 의해, 다음과 같이, 다른 종료조건을 갖는 마르코프 결정과정을 생각할 수 있다.

(A) 목표가 존재해서, 목표의 상태에 도달하면 종료

(B) 미리 정해진 시간이 길이에 도달하면 종료

(C) 종료하지 않는다(무한시간의 마르코프 결정과정)







