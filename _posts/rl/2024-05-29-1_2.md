---
title:  "강화학습을 읽고 ... 1. 준비(2)"
excerpt: "모리무라 선생님의 강화학습을 읽고 요약하기 위한 포스트입니다."

categories:
  - rl
tags:
  - [강화학습, TIL]

toc: true
toc_sticky: true
 
date: 2024-05-29
last_modified_at: 2024-05-29
---

# 1.2. 마르코프 결정 과정과 순차적 의사 결정 문제

## 1.2.2. 마르코프 결정 과정

__확률제어과정__(stochastic cotrol process) : __상태__(state)만 다루는 확률과정이 아닌, __행동__(action) 등을 추가한 **확률과정**이다.

강화학습에선 행동 선택 룰의 최적화를 구해야하므로, 확률제어과정을 알아보도록 하자.

__마르코프 결정과정__(Markov decision process; MDP) : $M \triangleq {\\{S, A, p_{s_{0}}, p_{\mathsf{T}}, g\\}}$. 마르코프 체인에, __행동__(action)과 어떤 상태에서 고른 행동에 따른 __보수__(reward)를 추가한 **확률제어과정**이다.

마르코프 결정과정 M의 요소는 다음과 같다.

$ \bullet $유한상태집합 $\mathcal{S} \triangleq {\\{1, \dots, \vert \mathcal{S} \vert \\}} \ni s$

$ \bullet $ 유한행동집합 $\mathcal{A} \triangleq {\\{a^1, \dots, a^{\vert \mathcal{A} \vert}\\}} \ni a $

$ \bullet $ 초기상태확률함수 $p_{s_{0}}: \mathcal{S} \rightarrow [0,1] , p_{s_{0}}(s) \triangleq Pr(S_{0}=s)$

$ \bullet $ 상태전이확률함수 $p_{\mathsf{T}}: \mathcal{S} \times  \mathcal{S} \times \mathcal{A}  \rightarrow [0,1], p_{ \mathsf{T}}(s' \mid s,a)  \triangleq Pr(S_{t+1} = s'  \mid S_t = s, A_t = a),  \forall t  \in  \mathbb{N}_0 $

$  \bullet $ 보수함수 $g :  \mathcal{S}  \times  \mathcal{A}  \rightarrow  \mathbb{R}$

(여기서, 유한상태집합, 유한행동집합에 대한 이산 시간 마르코프 결정과정을 주로 다루지만, 연속상태공간, 연속행동공간, 연속시간의 마르코프 결정과정에 관한 강화학습의 연구도 활발히 이루어지고 있다. )

__마르코프 보수과정__(Markov reward process) : 마르코프 체인에 보수만 추가한 마르코프 과정, 혹은 $ \mathcal{A} = 1 $인 마르코프 결정과정을 의미한다.

보수 함수$g$의 정의역이 유한 집합의 곱집합이므로, 그에 상응하는 치역도 유한 집합이며, 이는 보수 함수$g$가 유계 함수임을 뜻한다. 따라서, 

$ \exists R_{max} \in \mathbb{R} $ s.t. 

$ \forall (s,a) \in \mathcal{S} \times \mathcal{A} \vert g(s, a) \vert \leqq R_{max} $ 

가 성립한다.

$ \bullet $보수 집합 $\mathcal{R} \triangleq {\\{r \in \mathbb{R} : r = g(s,a), \exists (s,a) \in \mathcal{S} \times \mathcal{A} \\}} $

정의로부터 $ \vert \mathcal{R} \vert \leqq \vert \mathcal{S} \vert \vert \mathcal{A} \vert $ 가 성립한다.

앞으로 정리할 개념을 알기 쉽게 하기 위해 행동과 보수 등을 재정의하면, 

$ \bullet $ 행동집합 $A$ : 상태 $s$ 에 의존하지 않는 단일의 집합.

 만약 상태 $s$에 의해 선택 가능한 행동집합 $ A_s $가 모든 다른 경우에도

$$ \mathcal{A}_1 \cup \dots \cup \mathcal{A}_{\vert \mathcal{S} \vert} = \mathcal{A} $$

라 하면, 앞으로 문제없이 적용가능하다.

$ \bullet $ 보수함수 $g(S_t, A_t) $ :  현 시각 $t$에서의 상태, 행동 $(s,a)$에만 의존하는 함수.

보다 일반적으론 행동 선택 뒤의 상태에도 의존하는 $ g'(S_t,A_t,S_{t+1}) $나 보수분포함수 $Pr(R_t \leqq r \mid S_t = s, A_t = a)$ 등을 이용하는 경우도 있지만, 대부분의 경우, 보수함수 $g$와 같은 것으로 취급이 가능하다. 

__손실__(cost) : 보수에 음의 기호 '$-$'를 붙인 것

__방책__(policy) : 마르코프 결정과정에서의 입력이 되는 행동을 선택하는 법칙을 규정하는 함수.

앞으로 다루게 될 방책 함수는 특별한 경우가 아니고서는, 현 시각 $t$에 대한 상태 $s$에만 의존하여 확률적으로 행동 $a$를 선택하는 

__확률적 방책__(stochastic policy) $ \pi : \mathcal{A} \times \mathcal{S} \rightarrow [0,1] , \pi(a \mid s) \triangleq Pr(A=a \mid S=s) $ 

을 이용한다.

$ \bullet $ 방책집합 $\Pi \triangleq $  $\\{\pi : \mathcal{A} \times \mathcal{S} \rightarrow [0,1]  :  \sum\limits_{a \in \mathcal{A}} \pi(a \mid s) = 1, \forall s \in \mathcal{S}\\}$

$\bullet$ 마르코프 결정과정 $ M(\pi) \triangleq $ $\\{\mathcal{S}, \mathcal{A}, p_{s_0}, p_{\mathsf{T}}, g, \pi\\}$
(방책 $\pi$를 포함한 마르코프 결정과정을 재정의한 것이다.)

$\bullet$ 마르코프 결정과정 $M(\pi)=$ $\\{\mathcal{S}, \mathcal{A}, p_{s_0}, p_{\mathsf{T}},g,\pi\\}$의 시간 전개 $(s_0,a_0,r_0, \dots,s_t,a_t,r_t)$의 알고리즘

1. 시간 $t$를 0으로 초기화, 초기상태확률 $p_{s_0}$에 따른 초기상태 







